{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import feedparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Base api query url\n",
    "base_url = 'http://export.arxiv.org/api/query?';\n",
    "\n",
    "# Search parameters\n",
    "search_query = 'cat:cs.NI&sortBy=submittedDate&sortOrder=descending' # topic\n",
    "start = 0                     \n",
    "max_results = 100\n",
    "test_df = pd.DataFrame()\n",
    "arxivid=[]\n",
    "dop = []\n",
    "paper_title = []\n",
    "all_authors = []\n",
    "ref = []\n",
    "\n",
    "\n",
    "while(start<5000):\n",
    "    query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
    "                                                         start,\n",
    "                                                         max_results)\n",
    "    start = start + 100\n",
    "    # Opensearch metadata such as totalResults, startIndex, \n",
    "    # and itemsPerPage live in the opensearch namespase.\n",
    "    # Some entry metadata lives in the arXiv namespace.\n",
    "    # This is a hack to expose both of these namespaces in\n",
    "    # feedparser v4.1\n",
    "    feedparser._FeedParserMixin.namespaces['http://a9.com/-/spec/opensearch/1.1/'] = 'opensearch'\n",
    "    feedparser._FeedParserMixin.namespaces['http://arxiv.org/schemas/atom'] = 'arxiv'\n",
    "\n",
    "    # perform a GET request using the base_url and query\n",
    "    with urlopen(base_url+query) as url:\n",
    "        response = url.read()\n",
    "\n",
    "    print(base_url+query)\n",
    "    # response = urllib.urlopen(base_url+query).read()\n",
    "\n",
    "    # parse the response using feedparser\n",
    "    feed = feedparser.parse(response)\n",
    "\n",
    "    # print out feed information\n",
    "    # print 'Feed title: %s' % feed.feed.title\n",
    "    # print 'Feed last updated: %s' % feed.feed.updated\n",
    "\n",
    "    # Run through each entry, and print out information\n",
    "    for entry in feed.entries:\n",
    "#         print ('e-print metadata')\n",
    "#         print ('arxiv-id: %s' % entry.id.split('/abs/')[-1])\n",
    "        arxivid.append(entry.id.split('/abs/')[-1])\n",
    "        \n",
    "#         print ('Published: %s' % entry.published)\n",
    "        dop.append(entry.published)\n",
    "        \n",
    "#         print ('Title:  %s' % entry.title)\n",
    "        paper_title.append(entry.title)\n",
    "\n",
    "        # feedparser v4.1 only grabs the first author\n",
    "        author_string = entry.author\n",
    "\n",
    "        # grab the affiliation in <arxiv:affiliation> if present\n",
    "        # - this will only grab the first affiliation encountered\n",
    "        #   (the first affiliation for the first author)\n",
    "        # Please email the list with a way to get all of this information!\n",
    "#         try:\n",
    "#             author_string += ' (%s)' % entry.arxiv_affiliation\n",
    "#         except AttributeError:\n",
    "#             pass\n",
    "\n",
    "#         print ('Last Author:  %s' % author_string)\n",
    "\n",
    "        # feedparser v5.0.1 correctly handles multiple authors, print them all\n",
    "        try:\n",
    "#             print ('Authors:  %s' % ', '.join(author.name for author in entry.authors))\n",
    "            authors = []\n",
    "            for author in entry.authors:\n",
    "                authors.append(author.name)\n",
    "#             print('Authors')\n",
    "#             print(authors)\n",
    "            all_authors.append(authors)\n",
    "        except AttributeError:\n",
    "            all_authors.append(authors)\n",
    "            pass\n",
    "        \n",
    "        # get the links to the abs page and pdf for this e-print\n",
    "    #     for link in entry.links:\n",
    "    #         if link.rel == 'alternate':\n",
    "    #             print 'abs page link: %s' % link.href\n",
    "    #         elif link.title == 'pdf':\n",
    "    #             print 'pdf link: %s' % link.href\n",
    "\n",
    "        # The journal reference, comments and primary_category sections live under \n",
    "        # the arxiv namespace\n",
    "        try:\n",
    "            journal_ref = entry.arxiv_journal_ref\n",
    "        except AttributeError:\n",
    "            journal_ref = 'NA'\n",
    "#         print ('Journal reference: %s' % journal_ref)\n",
    "        ref.append(journal_ref)\n",
    "\n",
    "#         try:\n",
    "#             comment = entry.arxiv_comment\n",
    "#         except AttributeError:\n",
    "#             comment = 'No comment found'\n",
    "#         print ('Comments: %s' % comment)\n",
    "\n",
    "        # Since the <arxiv:primary_category> element has no data, only\n",
    "        # attributes, feedparser does not store anything inside\n",
    "        # entry.arxiv_primary_category\n",
    "        # This is a dirty hack to get the primary_category, just take the\n",
    "        # first element in entry.tags.  If anyone knows a better way to do\n",
    "        # this, please email the list!\n",
    "#         print ('Primary Category: %s' % entry.tags[0]['term'])\n",
    "\n",
    "        # Lets get all the categories\n",
    "        all_categories = [t['term'] for t in entry.tags]\n",
    "#         print(all_categories)\n",
    "#         print ('All Categories: %s' % (', ').join(all_categories))\n",
    "\n",
    "        # The abstract is in the <summary> element\n",
    "    #     print ('Abstract: %s' %  entry.summary)\n",
    "    \n",
    "    \n",
    "    \n",
    "test_df['arxiv_id'] = arxivid\n",
    "test_df['title'] = paper_title\n",
    "test_df['dop'] = dop\n",
    "test_df['authors'] = all_authors\n",
    "test_df['ref'] = ref\n",
    "\n",
    "test_df.head()\n",
    "# test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test_data_arxiv_networking.csv', index = False)\n",
    "# stat.CO statistics\n",
    "# cs.DC distributed\n",
    "# cs.HC Human Computer Interaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
